{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2YAgMWblLPK4k7CY9M7am",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prgawade/battlefieldofAI/blob/main/Assignment_Pytorch2.5_final\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the neural network pipeline:\n",
        "1. Prepare the data\n",
        "2. Build the model\n",
        "3. Train the model\n",
        "4. Analyze the model"
      ],
      "metadata": {
        "id": "O5db7fFmlW0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "35auQ6Lok14e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision # provide access to datasets, models, transforms, utils, etc\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wTfij9cVlTRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Defining a custom dataset  \n",
        "class modelData():\n",
        "  def __init__(self, Dataset):\n",
        "    self.data1 = Dataset\n",
        "    ### generating a random tensor having value 0 to 9\n",
        "    random_tensor = torch.randint(0,9,(10,))\n",
        "    #print (random_tensor)\n",
        "    ##### one hot encoding of the tensor to 10 classess\n",
        "    self.data2 =  torch.nn.functional.one_hot(random_tensor, num_classes= 10)\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    ##### get the image and label from the dataset , in this case MNIST\n",
        "    img, lbl = self.data1.__getitem__(index)\n",
        "    #print(\"lablel is\", lbl)\n",
        "    lbl = np.array(lbl)\n",
        "    lbl = torch.tensor(lbl)\n",
        "    #lbl = lbl.long()\n",
        "    #### one hot encoding of the MNIST label\n",
        "    lbl = torch.nn.functional.one_hot(lbl, num_classes= 10)\n",
        "    #lbl = lbl.float()\n",
        "    #### passing random values equivalent to the size of dataset 1\n",
        "    randomint = self.data2[index % 10]\n",
        "    #### concatenate two tensors\n",
        "    sum = torch.cat((lbl, randomint)).float()\n",
        "    #sum = 0\n",
        "    return img, lbl , randomint, sum \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.data1.__len__()\n"
      ],
      "metadata": {
        "id": "TU8zOc2WlSJv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### downloading the MNIST dataset\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root='./data'\n",
        "    ,train=True\n",
        "    ,download=True\n",
        "    ,transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n"
      ],
      "metadata": {
        "id": "B4Ze-_c-iwKc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### IGNORE - Testing data set\n",
        "dataset = modelData(train_set)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "next(iter(loader))[3]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaM1FzLTs1aj",
        "outputId": "b061a499-da17-4dcf-f951-352e3b387569"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### IGNORE - Testing data set\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
        "for batch in train_loader:\n",
        "  labels, images, numbers, sum = batch\n",
        "  #print(images)\n",
        "  #print(labels[1])\n",
        "  print(numbers)\n",
        "  #print(sum[1])\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO8gs3ah1cmz",
        "outputId": "7a2e7731-49c3-45e7-f1d7-411baa380604"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Model architecture\n",
        "\n",
        "import torch.nn as nn\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "    self.out = nn.Linear(in_features=60, out_features=10)\n",
        "    ### out features for MNIST = 10 + in features of Random INT = 10\n",
        "    self.fc3 = nn.Linear(20, 100)\n",
        "    self.fc4 = nn.Linear(100, 30)\n",
        "    self.fc5 = nn.Linear(30, 20) #### Output feature size is 20 after concatenating one hot encoded label and the random integer\n",
        "  \n",
        "  def forward(self, image, randnum):\n",
        "    # input layer\n",
        "    x1 = image\n",
        "    x2 = randnum\n",
        "    # conv1 layer\n",
        "    x1 = self.conv1(x1) # 28 > 24\n",
        "    x1 = F.relu(x1)\n",
        "    x1 = F.max_pool2d(x1, kernel_size=2, stride=2) # 28 | 24 | 12\n",
        "    # conv2 layer\n",
        "    x1 = self.conv2(x1) # 12\n",
        "    x1 = F.relu(x1) # 8\n",
        "    x1 = F.max_pool2d(x1, kernel_size=2, stride=2) # 12 | 8 | 4 >> 12x4x4\n",
        "    # reshapre\n",
        "    x1 = x1.reshape(-1, 12 * 4 * 4)\n",
        "    # fc1 layer\n",
        "    x1 = self.fc1(x1)\n",
        "    x1 = F.relu(x1)\n",
        "    # fc2 layer\n",
        "    x1 = self.fc2(x1)\n",
        "    x1 = F.relu(x1)\n",
        "    # output layer\n",
        "    x1 = self.out(x1)\n",
        "    # x = F.softmax(x, dim=1)\n",
        "    ##### concatenating two inputs on dimension 1\n",
        "    x = torch.cat((x1, x2), dim=1)\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = F.relu(self.fc5(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "L5gTwcTpggUq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_correct(preds, sumvalue):\n",
        "  return preds.argmax(dim=1).eq(sumvalue.argmax(dim=1)).sum().item()"
      ],
      "metadata": {
        "id": "W3Q3F6ob1ern"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "NeIkOdX2zfiD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### IGNORE THIS SECTION ######\n",
        "\"\"\"dataset = modelData(train_set)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "data = next(iter(loader))\n",
        "img = data[0]\n",
        "number = data[2]\n",
        "sum = data[3]\n",
        "network1 = Network()\n",
        "preds = network1(img,number)\n",
        "print(\"argmax for prediction \" , preds.argmax(dim=1))\n",
        "print(\"argmax for sum \", sum.argmax(dim=1))\n",
        "print(\"compare of two value\", preds.argmax(dim=1).eq(sum.argmax(dim=1)).sum())\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "t0edKsk81eQD",
        "outputId": "1b991c82-5d3e-4f29-efa1-0fc4971d181c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bbd584850c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnetwork1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"argmax for prediction \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"argmax for sum \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a0d17d7019d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, randnum)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# conv1 layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 28 > 24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 28 | 24 | 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# conv2 layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hSvcVYze1dzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Network()\n",
        "### Moving to model to cuda\n",
        "network.to(device)\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "\n",
        "#train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "  total_loss = 0\n",
        "  total_correct = 0\n",
        "  for batch in train_loader: # Get Batch\n",
        "      imgs , lbl, numbers, sum = batch \n",
        "      ### MOVING DATA TO GPU\n",
        "      imgs , lbl, numbers, sum = imgs.cuda(), lbl.cuda(), numbers.cuda(), sum.cuda()\n",
        "      preds = network(imgs,numbers) # Pass Batch\n",
        "      loss = F.cross_entropy(preds, sum) # Calculate Loss\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward() # Calculate Gradients\n",
        "      optimizer.step() # Update Weights\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      #print(\"shape of predictions and sum\", preds.shape, sum.shape)\n",
        "      total_correct += get_num_correct(preds, sum)\n",
        "\n",
        "  print(\n",
        "      \"epoch:\", epoch, \n",
        "      \"total_correct:\", total_correct, \n",
        "      \"loss:\", total_loss\n",
        "  )"
      ],
      "metadata": {
        "id": "UDnMtVyviRR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6846b1-0c4b-48a7-b763-d90029b03549"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 total_correct: 10165 loss: 9651.774094104767\n",
            "epoch: 1 total_correct: 10042 loss: 9282.353863716125\n",
            "epoch: 2 total_correct: 9893 loss: 9192.84495973587\n",
            "epoch: 3 total_correct: 9952 loss: 9194.14065694809\n",
            "epoch: 4 total_correct: 9804 loss: 9203.87129497528\n",
            "epoch: 5 total_correct: 9807 loss: 9194.506520748138\n",
            "epoch: 6 total_correct: 7919 loss: 9345.428928852081\n",
            "epoch: 7 total_correct: 4678 loss: 9608.603102207184\n",
            "epoch: 8 total_correct: 4687 loss: 9601.936371326447\n",
            "epoch: 9 total_correct: 4773 loss: 9617.778942584991\n"
          ]
        }
      ]
    }
  ]
}