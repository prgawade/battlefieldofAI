{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prgawade/battlefieldofAI/blob/main/S4/S4-Assignment-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "#PyTorch is an open source machine learning framework\n",
        "#PyTorch is a Python package that provides two high-level features:\n",
        "#Tensor computation (like NumPy) with strong GPU acceleration\n",
        "#Deep neural networks built on a tape-based autograd system\n",
        "import torch\n",
        "# A neural networks library deeply integrated with autograd designed for maximum flexibility\n",
        "import torch.nn as nn\n",
        "#Convolution functions such as conv1d , conv2d, conv3d\n",
        "import torch.nn.functional as F\n",
        "#torch.optim is a package implementing various optimization algorithms.\n",
        "# Adam - Implements Adam algorithm\n",
        "# SGD Implements stochastic gradient descent (optionally with momentum).\n",
        "import torch.optim as optim\n",
        "# The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "# Transforms are common image transformations available in the torchvision.transforms module. \n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=128,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(128, 8, 1, 1, 1),\n",
        "            nn.Conv2d(8, 16, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 16, 5, 1, 2),     \n",
        "            nn.ReLU(), \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.MaxPool2d(kernel_size=2), \n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(16, 8, 1, 1, 1)              \n",
        "        )\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(8 * 8 * 8, 10),                 \n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        ########### Applying global average pooling ########################\n",
        "        x = F.adaptive_avg_pool2d(x, (8, 8))\n",
        "        # flatten the output of conv2 to (batch_size, 8 * 8 * 8)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return  F.log_softmax(output)   # return x for visualization"
      ],
      "metadata": {
        "id": "JdiwCOu82dOA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c4aba6-1e71-4e9e-adca-cd9b8af3fbae"
      },
      "source": [
        "!pip install torchsummary\n",
        "#API to view the visualization of the model, which is helpful while debugging your network.\n",
        "from torchsummary import summary\n",
        "# Returns a bool indicating if CUDA is currently available.\n",
        "# NVIDIAâ€™s CUDA is a general purpose parallel computing platform and programming model that accelerates deep learning and other compute-intensive apps by taking advantage of the parallel processing power of GPUs.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# Specify which device to use\n",
        "model = Net().to(device)\n",
        "# Model summary\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 128, 28, 28]           3,328\n",
            "              ReLU-2          [-1, 128, 28, 28]               0\n",
            "       BatchNorm2d-3          [-1, 128, 28, 28]             256\n",
            "           Dropout-4          [-1, 128, 28, 28]               0\n",
            "            Conv2d-5            [-1, 8, 30, 30]           1,032\n",
            "            Conv2d-6           [-1, 16, 30, 30]           1,168\n",
            "              ReLU-7           [-1, 16, 30, 30]               0\n",
            "       BatchNorm2d-8           [-1, 16, 30, 30]              32\n",
            "           Dropout-9           [-1, 16, 30, 30]               0\n",
            "        MaxPool2d-10           [-1, 16, 15, 15]               0\n",
            "           Conv2d-11           [-1, 16, 15, 15]           6,416\n",
            "             ReLU-12           [-1, 16, 15, 15]               0\n",
            "      BatchNorm2d-13           [-1, 16, 15, 15]              32\n",
            "          Dropout-14           [-1, 16, 15, 15]               0\n",
            "        MaxPool2d-15             [-1, 16, 7, 7]               0\n",
            "           Conv2d-16             [-1, 16, 7, 7]           2,320\n",
            "             ReLU-17             [-1, 16, 7, 7]               0\n",
            "      BatchNorm2d-18             [-1, 16, 7, 7]              32\n",
            "          Dropout-19             [-1, 16, 7, 7]               0\n",
            "           Conv2d-20              [-1, 8, 9, 9]             136\n",
            "           Linear-21                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 19,882\n",
            "Trainable params: 19,882\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 3.73\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 3.81\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-f32fef0cdb4a>:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return  F.log_softmax(output)   # return x for visualization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "\n",
        "#Sets the seed for generating random numbers. Returns a torch.Generator object.\n",
        "\n",
        "torch.manual_seed(1)\n",
        "#batch_size = 128\n",
        "batch_size = 128\n",
        "#PyTorch's DataLoader class, which in addition to our Dataset class, also takes in the following important arguments:\n",
        "\n",
        "#batch_size, which denotes the number of samples contained in each generated batch.\n",
        "#shuffle. If set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
        "#num_workers, which denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e. that the bottleneck is indeed the neural network's forward and backward operations on the GPU (and not data generation).\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "# Download training data \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, \n",
        "                    transform=transforms.Compose([ # Composes several transforms together.\n",
        "                        transforms.ToTensor(), # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) # Normalize(mean, std[, inplace]) Normalize a tensor image with mean and standard deviation.\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# model defined above\n",
        "# device in this case will cpu\n",
        "# train loader to download training images and transform into tensor \n",
        "# SGD optimizer\n",
        "# The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero.\n",
        "        output = model(data)\n",
        "        # loss function\n",
        "        loss = F.nll_loss(output, target) # loss function - The negative log likelihood loss.\n",
        "        loss.backward() \n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16772f04-e8d3-4eec-bd47-9ad13fcccb9f"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "#torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False)\n",
        "#params (iterable) â€“ iterable of parameters to optimize or dicts defining parameter groups\n",
        "\n",
        "#lr (float) â€“ learning rate\n",
        "\n",
        "#momentum (float, optional) â€“ momentum factor (default: 0)\n",
        "\n",
        "#weight_decay (float, optional) â€“ weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "#dampening (float, optional) â€“ dampening for momentum (default: 0)\n",
        "\n",
        "#nesterov (bool, optional) â€“ enables Nesterov momentum (default: False)\n",
        "\n",
        "#maximize (bool, optional) â€“ maximize the params based on the objective, instead of minimizing (default: False)\n",
        "\n",
        "#foreach (bool, optional) â€“ whether foreach implementation of optimizer is used (default: None)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "epochs = 20\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print('epoch value is , ', epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch value is ,  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-28-f32fef0cdb4a>:52: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return  F.log_softmax(output)   # return x for visualization\n",
            "loss=0.1726485937833786 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:20<00:00, 23.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0986, Accuracy: 9691/10000 (97%)\n",
            "\n",
            "epoch value is ,  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.02403418719768524 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0323, Accuracy: 9902/10000 (99%)\n",
            "\n",
            "epoch value is ,  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.027550719678401947 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0283, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch value is ,  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0842796340584755 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 28.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0300, Accuracy: 9896/10000 (99%)\n",
            "\n",
            "epoch value is ,  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0314621664583683 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 27.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0230, Accuracy: 9921/10000 (99%)\n",
            "\n",
            "epoch value is ,  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.02799431048333645 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 28.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch value is ,  7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0137522853910923 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 26.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0231, Accuracy: 9929/10000 (99%)\n",
            "\n",
            "epoch value is ,  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.01810109056532383 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 26.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 9930/10000 (99%)\n",
            "\n",
            "epoch value is ,  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.09114695340394974 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 26.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9927/10000 (99%)\n",
            "\n",
            "epoch value is ,  10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.036803558468818665 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0207, Accuracy: 9937/10000 (99%)\n",
            "\n",
            "epoch value is ,  11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.03330874443054199 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 27.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 9944/10000 (99%)\n",
            "\n",
            "epoch value is ,  12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.026055367663502693 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 26.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 9928/10000 (99%)\n",
            "\n",
            "epoch value is ,  13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.004545198287814856 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:17<00:00, 27.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0180, Accuracy: 9938/10000 (99%)\n",
            "\n",
            "epoch value is ,  14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.004932833835482597 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0225, Accuracy: 9932/10000 (99%)\n",
            "\n",
            "epoch value is ,  15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0004928396665491164 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 9944/10000 (99%)\n",
            "\n",
            "epoch value is ,  16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0180299561470747 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0176, Accuracy: 9948/10000 (99%)\n",
            "\n",
            "epoch value is ,  17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0023433775641024113 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0194, Accuracy: 9947/10000 (99%)\n",
            "\n",
            "epoch value is ,  18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.003397254506126046 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0175, Accuracy: 9941/10000 (99%)\n",
            "\n",
            "epoch value is ,  19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0004057574551552534 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 27.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 9946/10000 (99%)\n",
            "\n",
            "epoch value is ,  20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.00013964359823148698 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:16<00:00, 28.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 9934/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": 32,
      "outputs": []
    }
  ]
}