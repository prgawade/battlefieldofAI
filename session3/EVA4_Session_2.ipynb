{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prgawade/battlefieldofAI/blob/main/session3/EVA4_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "#PyTorch is an open source machine learning framework\n",
        "#PyTorch is a Python package that provides two high-level features:\n",
        "#Tensor computation (like NumPy) with strong GPU acceleration\n",
        "#Deep neural networks built on a tape-based autograd system\n",
        "import torch\n",
        "# A neural networks library deeply integrated with autograd designed for maximum flexibility\n",
        "import torch.nn as nn\n",
        "#Convolution functions such as conv1d , conv2d, conv3d\n",
        "import torch.nn.functional as F\n",
        "#torch.optim is a package implementing various optimization algorithms.\n",
        "# Adam - Implements Adam algorithm\n",
        "# SGD Implements stochastic gradient descent (optionally with momentum).\n",
        "import torch.optim as optim\n",
        "# The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "# Transforms are common image transformations available in the torchvision.transforms module. \n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "outputId": "fa539bbd-a9cd-4d73-c782-896b84b3a77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\n",
        "        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 28 X 28 X 32 # Output - 28 X 28 X 64 # output  RF - 5\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.selfconv = nn.Conv2d(64, 16, 1, padding=1)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input - 28 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\n",
        "        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\n",
        "        self.conv5 = nn.Conv2d(64, 128, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \n",
        "        self.conv6 = nn.Conv2d(128, 256, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\n",
        "        self.conv7 = nn.Conv2d(256, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1()\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv2(x))\n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool1(x)\n",
        "        \n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        #x   = self.selfconv(x)\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)\n",
        "\"\"\"\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\\n        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\\n        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\\n        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\\n        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \\n        self.conv1 = nn.Sequential(\\n            nn.Conv2d(1, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 28 X 28 X 32 # Output - 28 X 28 X 64 # output  RF - 5\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(32, 64, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(64),\\n            nn.Dropout(0.25)\\n        )\\n\\n        self.selfconv = nn.Conv2d(64, 16, 1, padding=1)\\n\\n        self.pool1 = nn.MaxPool2d(2, 2) #input - 28 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\\n        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\\n        self.conv3 = nn.Sequential(\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\\n        self.conv4 = nn.Sequential(\\n            nn.Conv2d(32, 64, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(64),\\n            nn.Dropout(0.25)\\n        )\\n        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\\n        self.conv5 = nn.Conv2d(64, 128, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \\n        self.conv6 = nn.Conv2d(128, 256, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\\n        self.conv7 = nn.Conv2d(256, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\\n\\n    def forward(self, x):\\n        #x = self.pool1()\\n        x  = F.relu(self.conv1(x))\\n\\n        x  = F.relu(self.conv2(x))\\n        x   = self.selfconv(x)\\n        x  = self.pool1(x)\\n        \\n        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\\n        #x   = self.selfconv(x)\\n        x = F.relu(self.conv6(F.relu(self.conv5(x))))\\n        x = self.conv7(x)\\n        x = x.view(-1, 10)\\n        return F.log_softmax(x)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\n",
        "        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \n",
        "        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \n",
        "            nn.Conv2d(1, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 26 X 26 X 32 # Output - 24 X 24 X 64 # output  RF - 5\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=1)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input - 26 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\n",
        "        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\n",
        "        self.conv5 = nn.Conv2d(32, 64, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \n",
        "        self.conv6 = nn.Conv2d(64, 128, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\n",
        "        self.conv7 = nn.Conv2d(128, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1()\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv2(x))\n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool1(x)\n",
        "        \n",
        "        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x  = F.relu(self.conv3(x))\n",
        "\n",
        "        x  = F.relu(self.conv4(x))\n",
        "        #x   = self.selfconv(x)\n",
        "        x  = self.pool2(x)\n",
        "                \n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rMC0fvU7edA5",
        "outputId": "2d8eb0db-e3eb-4748-d698-c173ee497781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\\n        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\\n        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\\n        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\\n        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \\n        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \\n            nn.Conv2d(1, 16, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(16),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 26 X 26 X 32 # Output - 24 X 24 X 64 # output  RF - 5\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n\\n        self.selfconv = nn.Conv2d(32, 8, 1, padding=1)\\n\\n        self.pool1 = nn.MaxPool2d(2, 2) #input - 26 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\\n        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\\n        self.conv3 = nn.Sequential(\\n            nn.Conv2d(8, 16, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(16),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\\n        self.conv4 = nn.Sequential(\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\\n        self.conv5 = nn.Conv2d(32, 64, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \\n        self.conv6 = nn.Conv2d(64, 128, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\\n        self.conv7 = nn.Conv2d(128, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\\n\\n    def forward(self, x):\\n        #x = self.pool1()\\n        x  = F.relu(self.conv1(x))\\n\\n        x  = F.relu(self.conv2(x))\\n        x   = self.selfconv(x)\\n        x  = self.pool1(x)\\n        \\n        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\\n        x  = F.relu(self.conv3(x))\\n\\n        x  = F.relu(self.conv4(x))\\n        #x   = self.selfconv(x)\\n        x  = self.pool2(x)\\n                \\n        x = F.relu(self.conv6(F.relu(self.conv5(x))))\\n        x = self.conv7(x)\\n        x = x.view(-1, 10)\\n        return F.log_softmax(x)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\n",
        "        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \n",
        "        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \n",
        "            nn.Conv2d(1, 16, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) \n",
        "        self.conv2 = nn.Sequential( ##input - 26 X 26 X 16 # Output - 24 X 24 X 32 # output  RF - 5\n",
        "            nn.Conv2d(16, 32, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=0) ##input - 24 X 24 X 32 # Output - 24 X 24 X 8 # output  RF - 5\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input - 24 X 24 X 8 # Output - 12 X 12 X 8 # output  RF - 10\n",
        "        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\n",
        "        self.conv3 = nn.Sequential( # input 12 X 12 X 8 # Output - 10 X 10 X 16 # output  RF - 12\n",
        "            nn.Conv2d(8, 16, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\n",
        "        self.conv4 = nn.Sequential( # input 10 X 10 X 16 # Output - 10 X 10 X 32 # output  RF - 14\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "             \n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=0) # input 10 X 10 X 32 # Output - 10 X 10 X 8 # output  RF - 28\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # input 10 X 10 X 8 # Output - 5 X 5 X 8 # output  RF - 28\n",
        "        self.conv5 = nn.Conv2d(8, 16, 3) # input 5 X 5 X 8 # Output - 3 X 3 X 16 # output  RF - \n",
        "        #self.conv6 = nn.Conv2d(16, 10, 3)# input 3 X 3 X 16 # Output - 1 X 1 X 10 # output  RF\n",
        "        self.conv6 = nn.Sequential( # input 3 X 3 X 16 # Output - 1 X 1 X 10 # output  RF\n",
        "            nn.Conv2d(16, 10, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv7 = nn.Conv2d(64, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1()\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv2(x))\n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool1(x)\n",
        "        \n",
        "        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv4(x))\n",
        "        #x = x.view(-1, 32)\n",
        "  \n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        #x = self.conv7(x)\n",
        "        #print (x.shape)\n",
        "        x = x.view(-1, 10)\n",
        "        #print (x.shape)\n",
        "        return F.log_softmax(x, dim = 1)"
      ],
      "metadata": {
        "id": "z7-nXeMaqBqI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003e3179-c889-49f6-b556-a06dd0dd6617"
      },
      "source": [
        "!pip install torchsummary\n",
        "#API to view the visualization of the model, which is helpful while debugging your network.\n",
        "from torchsummary import summary\n",
        "# Returns a bool indicating if CUDA is currently available.\n",
        "# NVIDIA’s CUDA is a general purpose parallel computing platform and programming model that accelerates deep learning and other compute-intensive apps by taking advantage of the parallel processing power of GPUs.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# Specify which device to use\n",
        "model = Net().to(device)\n",
        "# Model summary\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             160\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "           Dropout-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 32, 24, 24]           4,640\n",
            "              ReLU-6           [-1, 32, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 32, 24, 24]              64\n",
            "           Dropout-8           [-1, 32, 24, 24]               0\n",
            "            Conv2d-9            [-1, 8, 24, 24]             264\n",
            "        MaxPool2d-10            [-1, 8, 12, 12]               0\n",
            "           Conv2d-11           [-1, 16, 10, 10]           1,168\n",
            "             ReLU-12           [-1, 16, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 16, 10, 10]              32\n",
            "          Dropout-14           [-1, 16, 10, 10]               0\n",
            "           Conv2d-15           [-1, 32, 10, 10]           4,640\n",
            "             ReLU-16           [-1, 32, 10, 10]               0\n",
            "      BatchNorm2d-17           [-1, 32, 10, 10]              64\n",
            "          Dropout-18           [-1, 32, 10, 10]               0\n",
            "           Conv2d-19            [-1, 8, 10, 10]             264\n",
            "        MaxPool2d-20              [-1, 8, 5, 5]               0\n",
            "           Conv2d-21             [-1, 16, 3, 3]           1,168\n",
            "           Conv2d-22             [-1, 10, 1, 1]           1,450\n",
            "================================================================\n",
            "Total params: 13,946\n",
            "Trainable params: 13,946\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.09\n",
            "Params size (MB): 0.05\n",
            "Estimated Total Size (MB): 1.15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "\n",
        "#Sets the seed for generating random numbers. Returns a torch.Generator object.\n",
        "\n",
        "torch.manual_seed(1)\n",
        "#batch_size = 128\n",
        "batch_size = 128\n",
        "#PyTorch's DataLoader class, which in addition to our Dataset class, also takes in the following important arguments:\n",
        "\n",
        "#batch_size, which denotes the number of samples contained in each generated batch.\n",
        "#shuffle. If set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
        "#num_workers, which denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e. that the bottleneck is indeed the neural network's forward and backward operations on the GPU (and not data generation).\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "# Download training data \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, \n",
        "                    transform=transforms.Compose([ # Composes several transforms together.\n",
        "                        transforms.ToTensor(), # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) # Normalize(mean, std[, inplace]) Normalize a tensor image with mean and standard deviation.\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# model defined above\n",
        "# device in this case will cpu\n",
        "# train loader to download training images and transform into tensor \n",
        "# SGD optimizer\n",
        "# The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero.\n",
        "        output = model(data)\n",
        "        # loss function\n",
        "        loss = F.nll_loss(output, target) # loss function - The negative log likelihood loss.\n",
        "        loss.backward() \n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "e02e91b2-6d37-4e7e-9355-556b019f0b5b"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "#torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False)\n",
        "#params (iterable) – iterable of parameters to optimize or dicts defining parameter groups\n",
        "\n",
        "#lr (float) – learning rate\n",
        "\n",
        "#momentum (float, optional) – momentum factor (default: 0)\n",
        "\n",
        "#weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "#dampening (float, optional) – dampening for momentum (default: 0)\n",
        "\n",
        "#nesterov (bool, optional) – enables Nesterov momentum (default: False)\n",
        "\n",
        "#maximize (bool, optional) – maximize the params based on the objective, instead of minimizing (default: False)\n",
        "\n",
        "#foreach (bool, optional) – whether foreach implementation of optimizer is used (default: None)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print('epoch value is , ', epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "# Test set: Average loss: 0.3189, Accuracy: 8758/10000 (88%) - lr=0.01, momentum=0.9 , epoch 1, batch_size = 128\n",
        "# Test set: Average loss: 0.4987, Accuracy: 7927/10000 (79%) - lr=0.02, momentum=0.9 , epoch 1 , batch_size = 128\n",
        "# Test set: Average loss: 1.8482, Accuracy: 3000/10000 (30%) - lr=0.03, momentum=0.9 , epoch 1 , batch_size = 128\n",
        "# lr=0.01, momentum=0.9 , epoch 9\n",
        "# accuracy at epoch 3\n",
        "#Test set: Average loss: 0.0377, Accuracy: 9874/10000 (99%)\n",
        "\n",
        "# accuracy starts decreasing for same parameters even after run time restart\n",
        "#Test set: Average loss: 0.7211, Accuracy: 6943/10000 (69%) # lr=0.01, momentum=0.9 , epoch 4  , batch_size = 128\n",
        "\n",
        "### changing batch size to 16\n",
        "\n",
        "#Test set: Average loss: 1.4329, Accuracy: 4815/10000 (48%) # lr=0.01, momentum=0.9 , epoch 4  , batch_size = 16\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch value is ,  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.13724787533283234 batch_id=468: 100%|██████████| 469/469 [01:45<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0943, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "epoch value is ,  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.15356333553791046 batch_id=468: 100%|██████████| 469/469 [01:42<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0701, Accuracy: 9811/10000 (98%)\n",
            "\n",
            "epoch value is ,  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.22535967826843262 batch_id=28:   6%|▌         | 29/469 [00:08<02:08,  3.42it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-960818bde2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch value is , '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-2c1d785b2108>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss function - The negative log likelihood loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34mf'loss={loss.item()} batch_id={batch_idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}