{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prgawade/battlefieldofAI/blob/main/session3/EVA4_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "from __future__ import print_function\n",
        "#PyTorch is an open source machine learning framework\n",
        "#PyTorch is a Python package that provides two high-level features:\n",
        "#Tensor computation (like NumPy) with strong GPU acceleration\n",
        "#Deep neural networks built on a tape-based autograd system\n",
        "import torch\n",
        "# A neural networks library deeply integrated with autograd designed for maximum flexibility\n",
        "import torch.nn as nn\n",
        "#Convolution functions such as conv1d , conv2d, conv3d\n",
        "import torch.nn.functional as F\n",
        "#torch.optim is a package implementing various optimization algorithms.\n",
        "# Adam - Implements Adam algorithm\n",
        "# SGD Implements stochastic gradient descent (optionally with momentum).\n",
        "import torch.optim as optim\n",
        "# The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "# Transforms are common image transformations available in the torchvision.transforms module. \n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6dd1a744-9df2-4de6-a431-f9f9e8e15a15"
      },
      "source": [
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\n",
        "        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 28 X 28 X 32 # Output - 28 X 28 X 64 # output  RF - 5\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.selfconv = nn.Conv2d(64, 16, 1, padding=1)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input - 28 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\n",
        "        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\n",
        "        self.conv5 = nn.Conv2d(64, 128, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \n",
        "        self.conv6 = nn.Conv2d(128, 256, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\n",
        "        self.conv7 = nn.Conv2d(256, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1()\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv2(x))\n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool1(x)\n",
        "        \n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        #x   = self.selfconv(x)\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)\n",
        "\"\"\"\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\\n        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\\n        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\\n        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\\n        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \\n        self.conv1 = nn.Sequential(\\n            nn.Conv2d(1, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 28 X 28 X 32 # Output - 28 X 28 X 64 # output  RF - 5\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(32, 64, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(64),\\n            nn.Dropout(0.25)\\n        )\\n\\n        self.selfconv = nn.Conv2d(64, 16, 1, padding=1)\\n\\n        self.pool1 = nn.MaxPool2d(2, 2) #input - 28 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\\n        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\\n        self.conv3 = nn.Sequential(\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\\n        self.conv4 = nn.Sequential(\\n            nn.Conv2d(32, 64, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(64),\\n            nn.Dropout(0.25)\\n        )\\n        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\\n        self.conv5 = nn.Conv2d(64, 128, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \\n        self.conv6 = nn.Conv2d(128, 256, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\\n        self.conv7 = nn.Conv2d(256, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\\n\\n    def forward(self, x):\\n        #x = self.pool1()\\n        x  = F.relu(self.conv1(x))\\n\\n        x  = F.relu(self.conv2(x))\\n        x   = self.selfconv(x)\\n        x  = self.pool1(x)\\n        \\n        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\\n        #x   = self.selfconv(x)\\n        x = F.relu(self.conv6(F.relu(self.conv5(x))))\\n        x = self.conv7(x)\\n        x = x.view(-1, 10)\\n        return F.log_softmax(x)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\n",
        "        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \n",
        "        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \n",
        "            nn.Conv2d(1, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 26 X 26 X 32 # Output - 24 X 24 X 64 # output  RF - 5\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=1)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input - 26 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\n",
        "        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\n",
        "        self.conv5 = nn.Conv2d(32, 64, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \n",
        "        self.conv6 = nn.Conv2d(64, 128, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\n",
        "        self.conv7 = nn.Conv2d(128, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1()\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv2(x))\n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool1(x)\n",
        "        \n",
        "        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x  = F.relu(self.conv3(x))\n",
        "\n",
        "        x  = F.relu(self.conv4(x))\n",
        "        #x   = self.selfconv(x)\n",
        "        x  = self.pool2(x)\n",
        "                \n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "rMC0fvU7edA5",
        "outputId": "038d366e-9bd3-4d68-de9b-a1522ef21d36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\\n        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\\n        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\\n        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\\n        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \\n        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \\n            nn.Conv2d(1, 16, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(16),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input - 26 X 26 X 32 # Output - 24 X 24 X 64 # output  RF - 5\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n\\n        self.selfconv = nn.Conv2d(32, 8, 1, padding=1)\\n\\n        self.pool1 = nn.MaxPool2d(2, 2) #input - 26 X 28 X 64 # Output - 14 X 14 X 64 # output  RF - 10\\n        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\\n        self.conv3 = nn.Sequential(\\n            nn.Conv2d(8, 16, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(16),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\\n        self.conv4 = nn.Sequential(\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n        self.pool2 = nn.MaxPool2d(2, 2) # input 14 X 14 X 256 # Output - 7 X 7 X 256 # output  RF - 28\\n        self.conv5 = nn.Conv2d(32, 64, 3) # input 7 X 7 X 256 # Output - 5 X 5 X 512 # output  RF - \\n        self.conv6 = nn.Conv2d(64, 128, 3)# input 5 X 5 X 512 # Output - 3 X 3 X 1024 # output  RF\\n        self.conv7 = nn.Conv2d(128, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\\n\\n    def forward(self, x):\\n        #x = self.pool1()\\n        x  = F.relu(self.conv1(x))\\n\\n        x  = F.relu(self.conv2(x))\\n        x   = self.selfconv(x)\\n        x  = self.pool1(x)\\n        \\n        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\\n        x  = F.relu(self.conv3(x))\\n\\n        x  = F.relu(self.conv4(x))\\n        #x   = self.selfconv(x)\\n        x  = self.pool2(x)\\n                \\n        x = F.relu(self.conv6(F.relu(self.conv5(x))))\\n        x = self.conv7(x)\\n        x = x.view(-1, 10)\\n        return F.log_softmax(x)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\n",
        "        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\n",
        "        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \n",
        "        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \n",
        "            nn.Conv2d(1, 16, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) \n",
        "        self.conv2 = nn.Sequential( ##input - 26 X 26 X 16 # Output - 24 X 24 X 32 # output  RF - 5\n",
        "            nn.Conv2d(16, 32, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=0) ##input - 24 X 24 X 32 # Output - 24 X 24 X 8 # output  RF - 5\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input - 24 X 24 X 8 # Output - 12 X 12 X 8 # output  RF - 10\n",
        "        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\n",
        "        self.conv3 = nn.Sequential( # input 12 X 12 X 8 # Output - 10 X 10 X 16 # output  RF - 12\n",
        "            nn.Conv2d(8, 16, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\n",
        "        self.conv4 = nn.Sequential( # input 10 X 10 X 16 # Output - 10 X 10 X 32 # output  RF - 14\n",
        "            nn.Conv2d(16, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "             \n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=0) # input 10 X 10 X 32 # Output - 10 X 10 X 8 # output  RF - 28\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # input 10 X 10 X 8 # Output - 5 X 5 X 8 # output  RF - 28\n",
        "        self.conv5 = nn.Conv2d(8, 16, 3) # input 5 X 5 X 8 # Output - 3 X 3 X 16 # output  RF - \n",
        "        #self.conv6 = nn.Conv2d(16, 32, 3)# input 3 X 3 X 16 # Output - 1 X 1 X 32 # output  RF\n",
        "        #self.conv7 = nn.Conv2d(64, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\n",
        "        self.conv6 = nn.Sequential( # input 10 X 10 X 16 # Output - 10 X 10 X 32 # output  RF - 14\n",
        "            nn.Conv2d(16, 32, 3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1,1)),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(32, 16)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(16, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.pool1()\n",
        "        x  = F.relu(self.conv1(x))\n",
        "\n",
        "        x  = F.relu(self.conv2(x))\n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool1(x)\n",
        "        \n",
        "        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x  = F.relu(self.conv3(x))\n",
        "\n",
        "        x  = F.relu(self.conv4(x))\n",
        "        #x = x.view(-1, 32)\n",
        "  \n",
        "        x   = self.selfconv(x)\n",
        "        x  = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        #x = self.conv6(x)\n",
        "        #print (x.shape)\n",
        "        #x = x.view(-1, 64)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = x.view(-1, 10)\n",
        "        #print (x.shape)\n",
        "        return F.log_softmax(x)\n",
        "\t\t\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "z7-nXeMaqBqI",
        "outputId": "c32f0623-8223-49bf-e437-de39fd732841"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        # torch.nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.\\n        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\\n        # input channel is 1 , output channels 32 , number of kernels is 32, size of kernel is 3 X 3\\n        #28 X 28 X 1 | (3 X 3 X 1) X 32 | 26 X 26 X 32\\n        #self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input - 28 X 28 X 1 # Output - 28 X 28 X 32 # output  RF - 3 \\n        self.conv1 = nn.Sequential(   ##input - 28 X 28 X 1 # Output - 26 X 26 X 16 # output  RF - 3 \\n            nn.Conv2d(1, 16, 3, padding=0),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(16),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv2 = nn.Conv2d(32, 64, 3, padding=1) \\n        self.conv2 = nn.Sequential( ##input - 26 X 26 X 16 # Output - 24 X 24 X 32 # output  RF - 5\\n            nn.Conv2d(16, 32, 3, padding=0),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n\\n        self.selfconv = nn.Conv2d(32, 8, 1, padding=0) ##input - 24 X 24 X 32 # Output - 24 X 24 X 8 # output  RF - 5\\n\\n        self.pool1 = nn.MaxPool2d(2, 2) #input - 24 X 24 X 8 # Output - 12 X 12 X 8 # output  RF - 10\\n        #self.conv3 = nn.Conv2d(16, 32, 3, padding=1) # input 14 X 14 X 64 # Output - 14 X 14 X 128 # output  RF - 12\\n        self.conv3 = nn.Sequential( # input 12 X 12 X 8 # Output - 10 X 10 X 16 # output  RF - 12\\n            nn.Conv2d(8, 16, 3, padding=0),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(16),\\n            nn.Dropout(0.25)\\n        )\\n        #self.conv4 = nn.Conv2d(32, 64, 3, padding=1) # input 14 X 14 X 128 # Output - 14 X 14 X 256 # output  RF - 14\\n        self.conv4 = nn.Sequential( # input 10 X 10 X 16 # Output - 10 X 10 X 32 # output  RF - 14\\n            nn.Conv2d(16, 32, 3, padding=1),\\n            nn.ReLU(),\\n            nn.BatchNorm2d(32),\\n            nn.Dropout(0.25)\\n        )\\n\\n             \\n        self.selfconv = nn.Conv2d(32, 8, 1, padding=0) # input 10 X 10 X 32 # Output - 10 X 10 X 8 # output  RF - 28\\n        self.pool2 = nn.MaxPool2d(2, 2) # input 10 X 10 X 8 # Output - 5 X 5 X 8 # output  RF - 28\\n        self.conv5 = nn.Conv2d(8, 16, 3) # input 5 X 5 X 8 # Output - 3 X 3 X 16 # output  RF - \\n        #self.conv6 = nn.Conv2d(16, 32, 3)# input 3 X 3 X 16 # Output - 1 X 1 X 32 # output  RF\\n        #self.conv7 = nn.Conv2d(64, 10, 3) # input 3 X 3 X 1024 # Output - 1 X 1 X 10 # output  RF\\n        self.conv6 = nn.Sequential( # input 10 X 10 X 16 # Output - 10 X 10 X 32 # output  RF - 14\\n            nn.Conv2d(16, 32, 3, padding=0),\\n            nn.ReLU(),\\n            nn.AvgPool2d((1,1)),\\n            nn.Dropout(0.25)\\n        )\\n        self.fc1 = nn.Sequential(\\n            nn.Linear(32, 16)\\n        )\\n        self.fc2 = nn.Sequential(\\n            nn.Linear(16, 10)\\n        )\\n\\n    def forward(self, x):\\n        #x = self.pool1()\\n        x  = F.relu(self.conv1(x))\\n\\n        x  = F.relu(self.conv2(x))\\n        x   = self.selfconv(x)\\n        x  = self.pool1(x)\\n        \\n        #x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\\n        x  = F.relu(self.conv3(x))\\n\\n        x  = F.relu(self.conv4(x))\\n        #x = x.view(-1, 32)\\n  \\n        x   = self.selfconv(x)\\n        x  = self.pool2(x)\\n\\n        x = F.relu(self.conv3(x))\\n        x = F.relu(self.conv6(x))\\n        #x = self.conv6(x)\\n        #print (x.shape)\\n        #x = x.view(-1, 64)\\n        x = x.view(x.size(0), -1)\\n        x = self.fc1(x)\\n        x = self.fc2(x)\\n        x = x.view(-1, 10)\\n        #print (x.shape)\\n        return F.log_softmax(x)\\n\\t\\t\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=0,                  \n",
        "            ),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.MaxPool2d(kernel_size=2),     \n",
        "            nn.Dropout(0.25)    \n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 0),     \n",
        "            nn.ReLU(),                     \n",
        "            nn.BatchNorm2d(32),\n",
        "            #nn.MaxPool2d(kernel_size=2), \n",
        "            nn.Dropout(0.25), \n",
        "        )\n",
        "        self.selfconv = nn.Conv2d(32, 8, 1, padding=0)\n",
        "        # fully connected layer, output 10 classes\n",
        "\n",
        "        self.out1 = nn.Sequential(\n",
        "            nn.Linear(8 * 8 * 8, 10),      \n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.out2 = nn.Sequential(\n",
        "            nn.Linear(32, 10),          \n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        #print(x.shape)\n",
        "        x = self.selfconv(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (8, 8))\n",
        "        #print(x.shape)\n",
        "        #x = self.conv3(x)\n",
        "        #print(x.shape)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        #x = self.out1(x)\n",
        "        output = self.out1(x)\n",
        "        return  F.log_softmax(output)   # return x for visualization"
      ],
      "metadata": {
        "id": "JdiwCOu82dOA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80523298-b991-4527-f08c-cab5ae53a315"
      },
      "source": [
        "!pip install torchsummary\n",
        "#API to view the visualization of the model, which is helpful while debugging your network.\n",
        "from torchsummary import summary\n",
        "# Returns a bool indicating if CUDA is currently available.\n",
        "# NVIDIA’s CUDA is a general purpose parallel computing platform and programming model that accelerates deep learning and other compute-intensive apps by taking advantage of the parallel processing power of GPUs.\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "# Specify which device to use\n",
        "model = Net().to(device)\n",
        "# Model summary\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 24, 24]             416\n",
            "              ReLU-2           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-3           [-1, 16, 24, 24]              32\n",
            "           Dropout-4           [-1, 16, 24, 24]               0\n",
            "            Conv2d-5           [-1, 32, 20, 20]          12,800\n",
            "              ReLU-6           [-1, 32, 20, 20]               0\n",
            "       BatchNorm2d-7           [-1, 32, 20, 20]              64\n",
            "         MaxPool2d-8           [-1, 32, 10, 10]               0\n",
            "           Dropout-9           [-1, 32, 10, 10]               0\n",
            "           Conv2d-10            [-1, 8, 10, 10]             264\n",
            "           Linear-11                   [-1, 10]           5,130\n",
            "          Dropout-12                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 18,706\n",
            "Trainable params: 18,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.63\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 0.70\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-bc7f884a47af>:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return  F.log_softmax(output)   # return x for visualization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "\n",
        "#Sets the seed for generating random numbers. Returns a torch.Generator object.\n",
        "\n",
        "torch.manual_seed(1)\n",
        "#batch_size = 128\n",
        "batch_size = 128\n",
        "#PyTorch's DataLoader class, which in addition to our Dataset class, also takes in the following important arguments:\n",
        "\n",
        "#batch_size, which denotes the number of samples contained in each generated batch.\n",
        "#shuffle. If set to True, we will get a new order of exploration at each pass (or just keep a linear exploration scheme otherwise). Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. Doing so will eventually make our model more robust.\n",
        "#num_workers, which denotes the number of processes that generate batches in parallel. A high enough number of workers assures that CPU computations are efficiently managed, i.e. that the bottleneck is indeed the neural network's forward and backward operations on the GPU (and not data generation).\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "# Download training data \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, \n",
        "                    transform=transforms.Compose([ # Composes several transforms together.\n",
        "                        transforms.ToTensor(), # Convert a PIL Image or numpy.ndarray to tensor.\n",
        "                        transforms.Normalize((0.1307,), (0.3081,)) # Normalize(mean, std[, inplace]) Normalize a tensor image with mean and standard deviation.\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# model defined above\n",
        "# device in this case will cpu\n",
        "# train loader to download training images and transform into tensor \n",
        "# SGD optimizer\n",
        "# The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero.\n",
        "        output = model(data)\n",
        "        # loss function\n",
        "        loss = F.nll_loss(output, target) # loss function - The negative log likelihood loss.\n",
        "        loss.backward() \n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "fe756d93-3d7d-45cd-d158-6cde1dd0fd1b"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "#torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False)\n",
        "#params (iterable) – iterable of parameters to optimize or dicts defining parameter groups\n",
        "\n",
        "#lr (float) – learning rate\n",
        "\n",
        "#momentum (float, optional) – momentum factor (default: 0)\n",
        "\n",
        "#weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\n",
        "\n",
        "#dampening (float, optional) – dampening for momentum (default: 0)\n",
        "\n",
        "#nesterov (bool, optional) – enables Nesterov momentum (default: False)\n",
        "\n",
        "#maximize (bool, optional) – maximize the params based on the objective, instead of minimizing (default: False)\n",
        "\n",
        "#foreach (bool, optional) – whether foreach implementation of optimizer is used (default: None)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "epochs = 20\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print('epoch value is , ', epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "# Test set: Average loss: 0.3189, Accuracy: 8758/10000 (88%) - lr=0.01, momentum=0.9 , epoch 1, batch_size = 128\n",
        "# Test set: Average loss: 0.4987, Accuracy: 7927/10000 (79%) - lr=0.02, momentum=0.9 , epoch 1 , batch_size = 128\n",
        "# Test set: Average loss: 1.8482, Accuracy: 3000/10000 (30%) - lr=0.03, momentum=0.9 , epoch 1 , batch_size = 128\n",
        "# lr=0.01, momentum=0.9 , epoch 9\n",
        "# accuracy at epoch 3\n",
        "#Test set: Average loss: 0.0377, Accuracy: 9874/10000 (99%)\n",
        "\n",
        "# accuracy starts decreasing for same parameters even after run time restart\n",
        "#Test set: Average loss: 0.7211, Accuracy: 6943/10000 (69%) # lr=0.01, momentum=0.9 , epoch 4  , batch_size = 128\n",
        "\n",
        "### changing batch size to 16\n",
        "\n",
        "#Test set: Average loss: 1.4329, Accuracy: 4815/10000 (48%) # lr=0.01, momentum=0.9 , epoch 4  , batch_size = 16\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch value is ,  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-11-bc7f884a47af>:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return  F.log_softmax(output)   # return x for visualization\n",
            "loss=0.4433377683162689 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 24.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0609, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "epoch value is ,  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.3998001515865326 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "epoch value is ,  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.35732924938201904 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0426, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "epoch value is ,  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.34913170337677 batch_id=227:  49%|████▊     | 228/469 [00:07<00:08, 28.65it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-949309413c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch value is , '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-2c1d785b2108>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sets the gradients of all optimized torch.Tensor s to zero.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}